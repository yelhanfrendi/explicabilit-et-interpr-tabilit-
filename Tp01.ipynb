{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpLiGBUWCrtW",
        "outputId": "3fe747d2-3501-4fd8-e1a0-2db96cbea594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.15.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.35.91-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.15)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.27.0)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (5.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.8.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.5.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.6.0)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.47.1)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.36.0,>=1.35.91 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.35.91-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.17.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.25.5)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.91->boto3>=1.20.27->flair) (2.2.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (24.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading flair-0.15.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.35.91-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.35.91-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=f3c207c1a94033f94a4389e15c3214a5b064e690b932aedfde02a01e011ace2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=2e316aff48d36ef0b5c0bfe201db562f1819271ba6690508f1f66c54f7dfa7f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=f4a9358d000d60cc56c0776efe8d71b9546b90b8db98204b2700219bdea95c83\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=114c081e9617ca13643cb490e17e4ab6c79d951038e7decd9babcd3ed3a9be65\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6d65ef9774f772a4a6ae3a1b291d1385ce2c9c88790be096699bee779f5afeed\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=b0f933c1612c4cf8812c6abd5989be2e5b44b2693f58858d12d0bfc426637365\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, sortedcontainers, pptree, docopt, segtok, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, wikipedia-api, botocore, bioc, s3transfer, pytorch-revgrad, mpld3, boto3, transformer-smaller-training-vocab, flair\n",
            "Successfully installed bioc-2.1 boto3-1.35.91 botocore-1.35.91 conllu-4.5.3 docopt-0.6.2 flair-0.15.0 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.4 segtok-1.5.11 sortedcontainers-2.4.0 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.7.1\n"
          ]
        }
      ],
      "source": [
        "pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQdkawvAnsSK",
        "outputId": "184db07d-ce89-408b-972a-465d46ddffb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /root/nltk_data\n",
        "!python3 -m nltk.downloader -d /root/nltk_data punkt punkt_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMfAfVMknoFE",
        "outputId": "f32a3fe4-b5ad-498b-ce29-0053ee3802cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texte original :\n",
            "This is an example of text preprocessing! Let's clean it up.\n",
            "\n",
            "Texte prétraité :\n",
            "example text preprocessing let clean\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Télécharger les ressources NLTK nécessaires\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialisation des outils\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Fonction de prétraitement du texte\n",
        "def preprocess_text(text):\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "\n",
        "    # Supprimer la ponctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # Tokenisation (séparation des mots)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Suppression des stopwords et lemmatisation\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
        "    ]\n",
        "\n",
        "    # Rejoindre les tokens pour reformer le texte\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Exemple d'application du prétraitement\n",
        "sample_text = \"This is an example of text preprocessing! Let's clean it up.\"\n",
        "preprocessed_text = preprocess_text(sample_text)\n",
        "\n",
        "print(\"Texte original :\")\n",
        "print(sample_text)\n",
        "print(\"\\nTexte prétraité :\")\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDUf7nsM4A3G",
        "outputId": "6a44a6c5-1587-430c-f501-25f06699b656"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 171/171 [00:01<00:00, 95.65it/s] \n",
            "100%|██████████| 90/90 [00:00<00:00, 143.12it/s]\n",
            "100%|██████████| 88/88 [00:00<00:00, 151.53it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 165.56it/s]\n",
            "100%|██████████| 130/130 [00:01<00:00, 123.28it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 165.51it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 167.70it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 151.26it/s]\n",
            "100%|██████████| 66/66 [00:00<00:00, 170.61it/s]\n",
            "100%|██████████| 19/19 [00:00<00:00, 186.22it/s]\n",
            "100%|██████████| 55/55 [00:00<00:00, 128.17it/s]\n",
            "100%|██████████| 86/86 [00:00<00:00, 106.70it/s]\n",
            "100%|██████████| 233/233 [00:03<00:00, 69.27it/s]\n",
            "100%|██████████| 54/54 [00:00<00:00, 178.44it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 199.56it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 156.63it/s]\n",
            "100%|██████████| 54/54 [00:00<00:00, 179.63it/s]\n",
            "100%|██████████| 128/128 [00:01<00:00, 125.47it/s]\n",
            "100%|██████████| 65/65 [00:00<00:00, 166.01it/s]\n",
            "100%|██████████| 65/65 [00:00<00:00, 168.58it/s]\n",
            "100%|██████████| 183/183 [00:01<00:00, 95.10it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 148.53it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 174.11it/s]\n",
            "100%|██████████| 165/165 [00:01<00:00, 99.79it/s] \n",
            "100%|██████████| 73/73 [00:00<00:00, 151.06it/s]\n",
            "100%|██████████| 66/66 [00:00<00:00, 170.80it/s]\n",
            "100%|██████████| 286/286 [00:05<00:00, 54.99it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 156.02it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 174.20it/s]\n",
            "100%|██████████| 366/366 [00:07<00:00, 48.62it/s]\n",
            "100%|██████████| 251/251 [00:03<00:00, 63.17it/s]\n",
            "100%|██████████| 217/217 [00:03<00:00, 66.23it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 158.79it/s]\n",
            "100%|██████████| 378/378 [00:08<00:00, 45.72it/s]\n",
            "100%|██████████| 119/119 [00:00<00:00, 126.92it/s]\n",
            "100%|██████████| 184/184 [00:01<00:00, 97.58it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 192.68it/s]\n",
            "100%|██████████| 172/172 [00:01<00:00, 99.54it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 169.85it/s]\n",
            "100%|██████████| 139/139 [00:01<00:00, 117.98it/s]\n",
            "100%|██████████| 61/61 [00:00<00:00, 166.83it/s]\n",
            "100%|██████████| 123/123 [00:00<00:00, 123.43it/s]\n",
            "100%|██████████| 92/92 [00:00<00:00, 149.15it/s]\n",
            "100%|██████████| 144/144 [00:01<00:00, 98.41it/s]\n",
            "100%|██████████| 88/88 [00:00<00:00, 108.85it/s]\n",
            "100%|██████████| 114/114 [00:01<00:00, 92.67it/s]\n",
            "100%|██████████| 46/46 [00:00<00:00, 112.95it/s]\n",
            "100%|██████████| 73/73 [00:00<00:00, 157.70it/s]\n",
            "100%|██████████| 375/375 [00:07<00:00, 46.92it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 168.73it/s]\n",
            "100%|██████████| 241/241 [00:03<00:00, 64.92it/s]\n",
            "100%|██████████| 326/326 [00:05<00:00, 55.75it/s]\n",
            "100%|██████████| 96/96 [00:00<00:00, 144.57it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 177.54it/s]\n",
            "100%|██████████| 112/112 [00:00<00:00, 119.95it/s]\n",
            "100%|██████████| 113/113 [00:00<00:00, 126.99it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 143.48it/s]\n",
            "100%|██████████| 95/95 [00:00<00:00, 144.12it/s]\n",
            "100%|██████████| 307/307 [00:05<00:00, 55.69it/s]\n",
            "100%|██████████| 230/230 [00:02<00:00, 77.29it/s]\n",
            "100%|██████████| 73/73 [00:00<00:00, 164.37it/s]\n",
            "100%|██████████| 82/82 [00:00<00:00, 151.84it/s]\n",
            "100%|██████████| 45/45 [00:00<00:00, 179.91it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 181.33it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 127.64it/s]\n",
            "100%|██████████| 155/155 [00:01<00:00, 98.23it/s]\n",
            "100%|██████████| 198/198 [00:02<00:00, 74.27it/s]\n",
            "100%|██████████| 68/68 [00:00<00:00, 114.29it/s]\n",
            "100%|██████████| 107/107 [00:01<00:00, 91.58it/s]\n",
            "100%|██████████| 164/164 [00:01<00:00, 106.72it/s]\n",
            "100%|██████████| 119/119 [00:00<00:00, 121.17it/s]\n",
            "100%|██████████| 138/138 [00:01<00:00, 115.17it/s]\n",
            "100%|██████████| 130/130 [00:01<00:00, 115.77it/s]\n",
            "100%|██████████| 90/90 [00:00<00:00, 146.51it/s]\n",
            "100%|██████████| 162/162 [00:01<00:00, 96.27it/s]\n",
            "100%|██████████| 71/71 [00:00<00:00, 155.04it/s]\n",
            "100%|██████████| 174/174 [00:02<00:00, 73.65it/s]\n",
            "100%|██████████| 293/293 [00:05<00:00, 56.73it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 143.27it/s]\n",
            "100%|██████████| 153/153 [00:01<00:00, 113.08it/s]\n",
            "100%|██████████| 82/82 [00:00<00:00, 157.26it/s]\n",
            "100%|██████████| 77/77 [00:00<00:00, 163.38it/s]\n",
            "100%|██████████| 142/142 [00:01<00:00, 116.48it/s]\n",
            "100%|██████████| 229/229 [00:02<00:00, 76.98it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 183.56it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 130.26it/s]\n",
            "100%|██████████| 148/148 [00:01<00:00, 86.19it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 106.96it/s]\n",
            "100%|██████████| 114/114 [00:01<00:00, 110.47it/s]\n",
            "100%|██████████| 207/207 [00:02<00:00, 89.95it/s]\n",
            "100%|██████████| 86/86 [00:00<00:00, 146.48it/s]\n",
            "100%|██████████| 89/89 [00:00<00:00, 151.49it/s]\n",
            "100%|██████████| 208/208 [00:02<00:00, 88.37it/s]\n",
            "100%|██████████| 101/101 [00:00<00:00, 143.78it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 148.99it/s]\n",
            "100%|██████████| 142/142 [00:01<00:00, 112.00it/s]\n",
            "100%|██████████| 102/102 [00:00<00:00, 140.84it/s]\n",
            "100%|██████████| 75/75 [00:00<00:00, 135.45it/s]\n",
            "100%|██████████| 94/94 [00:00<00:00, 109.09it/s]\n",
            "100%|██████████| 278/278 [00:04<00:00, 56.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VP: 25 critiques\n",
            "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
            "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.\n",
            "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\n",
            "VN: 55 critiques\n",
            "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n",
            "This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\n",
            "Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\n",
            "FP: 3 critiques\n",
            "So im not a big fan of Boll's work but then again not many are. I enjoyed his movie Postal (maybe im the only one). Boll apparently bought the rights to use Far Cry long ago even before the game itself was even finsished. <br /><br />People who have enjoyed killing mercs and infiltrating secret research labs located on a tropical island should be warned, that this is not Far Cry... This is something Mr Boll have schemed together along with his legion of schmucks.. Feeling loneley on the set Mr Boll invites three of his countrymen to play with. These players go by the names of Til Schweiger, Udo Kier and Ralf Moeller.<br /><br />Three names that actually have made them selfs pretty big in the movie biz. So the tale goes like this, Jack Carver played by Til Schweiger (yes Carver is German all hail the bratwurst eating dudes!!) However I find that Tils acting in this movie is pretty badass.. People have complained about how he's not really staying true to the whole Carver agenda but we only saw carver in a first person perspective so we don't really know what he looked like when he was kicking a**.. <br /><br />However, the storyline in this film is beyond demented. We see the evil mad scientist Dr. Krieger played by Udo Kier, making Genetically-Mutated-soldiers or GMS as they are called. Performing his top-secret research on an island that reminds me of \"SPOILER\" Vancouver for some reason. Thats right no palm trees here. Instead we got some nice rich lumberjack-woods. We haven't even gone FAR before I started to CRY (mehehe) I cannot go on any more.. If you wanna stay true to Bolls shenanigans then go and see this movie you will not be disappointed it delivers the true Boll experience, meaning most of it will suck.<br /><br />There are some things worth mentioning that would imply that Boll did a good work on some areas of the film such as some nice boat and fighting scenes. Until the whole cromed/albino GMS squad enters the scene and everything just makes me laugh.. The movie Far Cry reeks of scheisse (that's poop for you simpletons) from a fa,r if you wanna take a wiff go ahead.. BTW Carver gets a very annoying sidekick who makes you wanna shoot him the first three minutes he's on screen.\n",
            "Caddyshack Two is a good movie by itself but compared to the original it cant stack up. Robert Stack is a horrible replacement for Ted Knight and Jackie Mason, while funny just cant compete with Rodney Dangerfield. Ty Webb is funny, being the only character from the original. Most of the other characters in the movie lack the punch of the original (Henry Wilcoxon for example) except for the hystericly funny lawyer Peter Blunt, being played by Randy Quaid. Every line he says reminds me of the originals humor, especially the scene at his office (I don't go in for law suits or motions. I find out where you live and come to your house and beat down your door with a f***ing baseball bat, make a bonfire with the chippindale,maybe roast that golden retriever (arff arff arff) then eat it. And then I'm comin' upstairs junior, and I'm grabbing you by your brooks brothers pjs, and cramming your brand new BMW up your tight a**! Do we have an understanding?). Offsetting his small role however, is Dan Acroyd, who is obviously no replacement for Bill Murray. His voice is beyond irritating and everything he does isnt even funny, its just stupid. Overall Caddyshack II is a good movie, but in comparison to the awesome original it just cant cut it.\n",
            "Upon viewing Tobe Hooper's gem, Crocodile, in 2000, I developed a great interest in the college/crocodile niche of the exploitation/monster genre. I look forward to a wayward producer to follow up with several sequels to these delightful bonbons of camp goodness. If only Ed Wood could bring his subtle sense of flair and dignity to these remarkable scripts. With Ed writing the scripts, and a room full of monkees creating crocodile special effects on a computer, all we'd need would be a cast of crocky fodder with Russ Meyer breasts and Ren Hoek pectoral implants.<br /><br />While Tobe Hooper's crocky opus referenced his own movies, Blood Surf chose to dish out a bunch of aging themes from the chum bucket of other movies. See if you can look past the Revenge of the Nerds sequel sets to find the allusions/homages?/rip-offs to Jaws, Temple of Doom, Indiana Jones' Last Crusade, The Convent, Godzilla 2000, and any James Bond movie. Also, try to find the ready-for-tv fade where the editor gave up on making sense of the stock.<br /><br />I was disappointed the crock didn't get to try out its sotto voce tenor with a soliloquy on environmentalism...or crocky appreciation, but the quasi-Captain Ahab of the story does get his tour de force speach. Perhaps, in the coming years, we'll see a crock galloping off after a shootout into a golden sunset. Or hopefully, a monkey will flush a crocky down the toilet of an international space station for midgets and enjoy the exploitative waltz of zero-G monkey/midget/crocodile bloodshed.<br /><br />All-in-all, the lack of a whammy bar in the surf music irked me.\n",
            "FN: 17 critiques\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Some films just simply should not be remade. This is one of them. In and of itself it is not a bad film. But it fails to capture the flavor and the terror of the 1963 film of the same title. Liam Neeson was excellent as he always is, and most of the cast holds up, with the exception of Owen Wilson, who just did not bring the right feel to the character of Luke. But the major fault with this version is that it strayed too far from the Shirley Jackson story in it's attempts to be grandiose and lost some of the thrill of the earlier film in a trade off for snazzier special effects. Again I will say that in and of itself it is not a bad film. But you will enjoy the friction of terror in the older version much more.\n",
            "I remember this film,it was the first film i had watched at the cinema the picture was dark in places i was very nervous it was back in 74/75 my Dad took me my brother & sister to Newbury cinema in Newbury Berkshire England. I recall the tigers and the lots of snow in the film also the appearance of Grizzly Adams actor Dan Haggery i think one of the tigers gets shot and dies. If anyone knows where to find this on DVD etc please let me know.The cinema now has been turned in a fitness club which is a very big shame as the nearest cinema now is 20 miles away, would love to hear from others who have seen this film or any other like it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import flair\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Télécharger les stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Chargement du modèle Flair\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "\n",
        "# Initialisation des structures de données\n",
        "polarities = dict()\n",
        "occurences = dict()\n",
        "categories = {\"VP\": [], \"VN\": [], \"FP\": [], \"FN\": []}\n",
        "\n",
        "# Fichier d'entrée et paramètres\n",
        "input_file = \"/content/IMDB-Dataset.100.csv\"\n",
        "output_file = \"/content/result_out.txt\"\n",
        "\n",
        "# Lecture et traitement des données\n",
        "with open(input_file, newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "    next(reader)\n",
        "    row_num = 0\n",
        "\n",
        "    for row in reader:\n",
        "        row_num += 1\n",
        "        critique = row[0]\n",
        "        true_label = row[1].strip().lower() \n",
        "\n",
        "      \n",
        "        filtered_critique = preprocess_text(critique)\n",
        "\n",
        "        s = flair.data.Sentence(filtered_critique)\n",
        "        flair_sentiment.predict(s)\n",
        "        predicted_label = s.labels[0].value.lower()\n",
        "        sentence_score = s.labels[0].score\n",
        "\n",
        "        # Catégorisation des critiques\n",
        "        if true_label == \"positive\" and predicted_label == \"positive\":\n",
        "            category = \"VP\"\n",
        "        elif true_label == \"negative\" and predicted_label == \"negative\":\n",
        "            category = \"VN\"\n",
        "        elif true_label == \"positive\" and predicted_label == \"negative\":\n",
        "            category = \"FN\"\n",
        "        elif true_label == \"negative\" and predicted_label == \"positive\":\n",
        "            category = \"FP\"\n",
        "        categories[category].append(critique)\n",
        "\n",
        "        # Analyse mot par mot\n",
        "        sent = [tok.text for tok in s]\n",
        "        for pos in tqdm(range(len(s.tokens))):\n",
        "            word = sent[pos]\n",
        "\n",
        "            if word not in polarities:\n",
        "                polarities[word] = 0  \n",
        "                occurences[word] = 0  \n",
        "\n",
        "            substracted = flair.data.Sentence(\" \".join(sent[:pos] + sent[pos + 1:]))\n",
        "            flair_sentiment.predict(substracted)\n",
        "            substracted_score = substracted.labels[0].score\n",
        "\n",
        "            # Calcul de delta\n",
        "            delta = sentence_score - substracted_score\n",
        "\n",
        "            # Changement de polarité\n",
        "            if s.labels[0].value != substracted.labels[0].value:\n",
        "                if s.labels[0].value == \"POSITIVE\":\n",
        "                    polarities[word] += 1\n",
        "                else:\n",
        "                    polarities[word] -= 1\n",
        "\n",
        "            else:  # Pas de changement de polarité\n",
        "                if s.labels[0].value == \"POSITIVE\":\n",
        "                    if delta < 0:\n",
        "                        polarities[word] += abs(delta)\n",
        "                    else:\n",
        "                        polarities[word] -= abs(delta)\n",
        "                else:\n",
        "                    if delta < 0:\n",
        "                        polarities[word] += abs(delta)\n",
        "                    else:\n",
        "                        polarities[word] -= abs(delta)\n",
        "\n",
        "            occurences[word] += 1  # Compter le nombre d'occurrences du mot\n",
        "\n",
        "# Moyenne des polarités par mot\n",
        "for word in polarities.keys():\n",
        "    polarities[word] /= occurences[word]\n",
        "\n",
        "# Tri des polarités\n",
        "sorted_polarities = dict(sorted(polarities.items(), key=lambda item: item[1]))\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for word, polarity in sorted_polarities.items():\n",
        "        outfile.write(f\"{word} {round(polarity, 3)} {occurences[word]}\\n\")\n",
        "\n",
        "for category, critiques in categories.items():\n",
        "    print(f\"{category}: {len(critiques)} critiques\")\n",
        "    print(\"\\n\".join(critiques[:3])) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSqMjwUhm38q",
        "outputId": "6b40c635-a2f3-4ce7-da11-8e8d92b24439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"it's\", 'ain', 'do', 'until', 'about', 'below', 'each', 'than', \"needn't\", 'against', \"you're\", 'themselves', 'she', 'can', 'myself', 'herself', 'haven', \"isn't\", 're', 'during', 'few', 'will', \"hasn't\", 'through', 'under', 'both', 'if', 'him', 'needn', 'had', \"wouldn't\", \"she's\", 'which', 'whom', \"doesn't\", \"aren't\", 'is', 't', 'there', 'on', 'only', \"shan't\", 'once', 'such', 'her', 'should', 'between', 'ours', 'having', 'was', 'me', \"mustn't\", 'here', 'all', 'mustn', 'by', 'being', 'then', 'shan', 'why', 'its', 'but', 'aren', 'ourselves', \"should've\", \"don't\", 'i', 'don', 'hasn', \"you'd\", 'further', 'own', 'o', 'from', \"you've\", 'other', 'at', 'wasn', 'a', \"you'll\", 'of', 'before', 'after', 'not', \"didn't\", 'again', 'now', 'did', 'what', 'that', 'shouldn', 'doing', 'how', 'didn', 'nor', 'so', 'couldn', 'doesn', 'out', 'isn', \"haven't\", 'are', \"that'll\", 'has', 'yourselves', 'down', 'who', 'those', 'the', 'he', 'to', 'more', 'hadn', \"won't\", 'no', 'as', 'does', 'for', 'an', 'too', 'our', 'y', 'their', 'and', 'have', 'over', 'any', 'some', 'were', 'into', 'ma', 'wouldn', 'am', 'very', 'it', 'd', 'we', 'or', 'while', \"weren't\", 'my', 'won', 'they', 'where', 'his', \"shouldn't\", 'you', 'because', 'hers', 'these', 'most', 'when', 'be', 've', 'your', 'mightn', 'above', 'just', \"mightn't\", 'been', 'll', 'yourself', 'off', 'this', \"couldn't\", 'itself', 'them', 'yours', 'himself', 'm', 's', 'weren', 'with', 'up', 'same', \"hadn't\", \"wasn't\", 'theirs', 'in'}\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN8sIkMUmBTI",
        "outputId": "b4938559-124d-4f41-c90a-4ed8df6ad23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de mots avec une polarité de 0 : 1800\n"
          ]
        }
      ],
      "source": [
        "count_zero_polarity = sum(1 for polarity in polarities.values() if polarity == 0)\n",
        "\n",
        "print(f\"Nombre de mots avec une polarité de 0 : {count_zero_polarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK-3Op6r4DC9",
        "outputId": "2a09f1ed-a601-41eb-a703-f249759d0c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mot avec la plus grande polarité :\n",
            "likable : 1.0\n",
            "\n",
            "Mot avec la plus petite polarité :\n",
            "biggest : -1.0\n",
            "\n",
            "20 mots avec la plus grande polarité :\n",
            "likable : 1.0\n",
            "johanson : 1.0\n",
            "resurrection : 1.0\n",
            "tech : 1.0\n",
            "ole : 1.0\n",
            "saywould : 1.0\n",
            "huntif : 1.0\n",
            "hr : 1.0\n",
            "liking : 1.0\n",
            "chiller : 1.0\n",
            "1940s : 1.0\n",
            "vanishes : 1.0\n",
            "ageing : 1.0\n",
            "discover : 1.0\n",
            "afterwardsbr : 1.0\n",
            "hag : 1.0\n",
            "hunchback : 1.0\n",
            "spooky : 1.0\n",
            "belas : 1.0\n",
            "coffin : 1.0\n",
            "\n",
            "20 mots avec la plus petite polarité :\n",
            "timer : -1.0\n",
            "maggot : -1.0\n",
            "douses : -1.0\n",
            "spoilersbr : -1.0\n",
            "rebecca : -1.0\n",
            "paranoid : -1.0\n",
            "jingoistic : -1.0\n",
            "deceptive : -1.0\n",
            "operator : -1.0\n",
            "property : -1.0\n",
            "coincidence : -1.0\n",
            "literary : -1.0\n",
            "maven : -1.0\n",
            "brief : -1.0\n",
            "returning : -1.0\n",
            "neglected : -1.0\n",
            "kasugi : -1.0\n",
            "johnrhys : -1.0\n",
            "badventure : -1.0\n",
            "biggest : -1.0\n"
          ]
        }
      ],
      "source": [
        "output_file = \"/content/result_out_clean_text.txt\"\n",
        "polarities = {}\n",
        "with open(output_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        word, polarity, occurrences = line.split()\n",
        "        polarities[word] = float(polarity)\n",
        "\n",
        "# Trier les polarités (du plus grand au plus petit)\n",
        "sorted_polarities = sorted(polarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "print(\"Mot avec la plus grande polarité :\")\n",
        "print(f\"{sorted_polarities[0][0]} : {sorted_polarities[0][1]}\")  \n",
        "print(\"\\nMot avec la plus petite polarité :\")\n",
        "print(f\"{sorted_polarities[-1][0]} : {sorted_polarities[-1][1]}\")  \n",
        "\n",
        "print(\"\\n20 mots avec la plus grande polarité :\")\n",
        "for word, polarity in sorted_polarities[:20]:\n",
        "    print(f\"{word} : {polarity}\")\n",
        "\n",
        "print(\"\\n20 mots avec la plus petite polarité :\")\n",
        "for word, polarity in sorted_polarities[-20:]:\n",
        "    print(f\"{word} : {polarity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqtDNSEg-pfU",
        "outputId": "63bda1e2-9c02-4f11-8edd-a00d8e88f573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le mot 'david' a une polarité de 0.007\n",
            "Le mot 'finney' a une polarité de 0.031\n",
            "Le mot 'johanson' a une polarité de 1.0\n",
            "Le mot 'rebecca' a une polarité de -1.0\n",
            "Le mot 'madge' a une polarité de 0.022\n",
            "Le mot 'adam' a une polarité de -0.153\n",
            "Le mot 'emma' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'james' a une polarité de 0.018\n",
            "Le mot 'jennifer' a une polarité de -0.042\n",
            "Le mot 'christopher' a une polarité de 0.17\n",
            "Le mot 'susan' a une polarité de 0.294\n",
            "Le mot 'john' a une polarité de -0.012\n",
            "Le mot 'mary' a une polarité de 0.0\n",
            "Le mot 'patrick' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'george' a une polarité de 0.003\n",
            "Le mot 'lisa' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'steve' a une polarité de -0.0\n",
            "Le mot 'robert' a une polarité de 0.049\n",
            "Le mot 'katherine' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'michael' a une polarité de 0.0\n",
            "Le mot 'paul' a une polarité de -0.0\n",
            "Le mot 'linda' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'andrew' a une polarité de 0.032\n",
            "Le mot 'joseph' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'nancy' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'richard' a une polarité de -0.0\n",
            "Le mot 'sarah' a une polarité de 0.004\n",
            "Le mot 'william' a une polarité de -0.002\n",
            "Le mot 'alexander' a une polarité de 0.292\n",
            "Le mot 'benjamin' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'elizabeth' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'kate' a une polarité de -0.0\n",
            "Le mot 'charles' a une polarité de 0.059\n",
            "Le mot 'victoria' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'sophie' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'natalie' a une polarité de -0.074\n",
            "Le mot 'thomas' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'hannah' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'will' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'alice' a une polarité de -0.109\n",
            "Le mot 'georgia' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'sophie' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'laura' n'a pas été trouvé dans le fichier.\n",
            "Le mot 'lucas' n'a pas été trouvé dans le fichier.\n"
          ]
        }
      ],
      "source": [
        "noms_a_verifier = [\n",
        "    \"david\", \"finney\", \"johanson\", \"rebecca\", \"madge\", \"adam\", \"emma\", \"james\", \"jennifer\",\n",
        "    \"christopher\", \"susan\", \"john\", \"mary\", \"patrick\", \"george\", \"lisa\", \"steve\", \"robert\",\n",
        "    \"katherine\", \"michael\", \"paul\", \"linda\", \"andrew\", \"joseph\", \"nancy\", \"richard\", \"sarah\",\n",
        "    \"william\", \"alexander\", \"benjamin\", \"elizabeth\", \"kate\", \"charles\", \"victoria\", \"sophie\",\n",
        "    \"natalie\", \"thomas\", \"hannah\", \"will\", \"alice\", \"georgia\", \"sophie\", \"laura\", \"lucas\"\n",
        "]\n",
        "\n",
        "\n",
        "polarites_verifiees = {}\n",
        "\n",
        "with open(\"/content/result_out_clean_text.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        word, polarity, occurrences = line.split()\n",
        "        word = word.lower()  \n",
        "\n",
        "        if word in noms_a_verifier:\n",
        "            polarites_verifiees[word] = float(polarity)\n",
        "\n",
        "# Afficher la polarité des mots vérifiés\n",
        "for nom in noms_a_verifier:\n",
        "    if nom in polarites_verifiees:\n",
        "        print(f\"Le mot '{nom}' a une polarité de {polarites_verifiees[nom]}\")\n",
        "    else:\n",
        "        print(f\"Le mot '{nom}' n'a pas été trouvé dans le fichier.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11be9RnLR4gP",
        "outputId": "ae4f0f84-6a1f-44d3-fd17-d80cec8e42dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le mot 'happy' a une polarité de 0.001\n",
            "Le mot 'sad' a une polarité de -0.0\n",
            "Le mot 'angry' a une polarité de -0.0\n",
            "Le mot 'love' a une polarité de 0.04\n",
            "Le mot 'hate' a une polarité de 0.011\n",
            "Le mot 'excited' a une polarité de 0.003\n",
            "Le mot 'bored' a une polarité de -0.0\n",
            "Le mot 'fear' a une polarité de 0.001\n",
            "Le mot 'surprised' a une polarité de -0.083\n",
            "Le mot 'nervous' a une polarité de -0.108\n",
            "Le mot 'confused' a une polarité de 0.0\n",
            "Le mot 'afraid' a une polarité de -0.499\n",
            "Le mot 'proud' a une polarité de 0.008\n",
            "Le mot 'trust' a une polarité de -0.005\n",
            "Le mot 'disappointed' a une polarité de 0.024\n",
            "Le mot 'desperate' a une polarité de -0.011\n",
            "Le mot 'unhappy' a une polarité de 0.013\n",
            "Le mot 'hopeless' a une polarité de 0.0\n",
            "Le mot 'gloomy' a une polarité de -0.02\n",
            "Le mot 'sympathetic' a une polarité de -0.002\n",
            "Le mot 'bitter' a une polarité de -0.001\n"
          ]
        }
      ],
      "source": [
        "words_to_compare = [\n",
        "    \"happy\", \"sad\", \"angry\", \"love\", \"hate\", \"excited\", \"bored\", \"fear\",\n",
        "    \"surprised\", \"nervous\",  \"confused\", \"afraid\",\n",
        "    \"proud\",  \"trust\", \"disappointed\",\"desperate\", \"unhappy\", \"hopeless\", \"gloomy\",\"sympathetic\", \"bitter\"\n",
        "]\n",
        "polarites_verifiees = {}\n",
        "\n",
        "with open(\"/content/result_out_clean_text.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        word, polarity, occurrences = line.split()\n",
        "        word = word.lower() \n",
        "\n",
        "        # Si le mot est dans la liste des mots à comparer, ajouter sa polarité\n",
        "        if word in words_to_compare:\n",
        "            polarites_verifiees[word] = float(polarity)\n",
        "\n",
        "for word in words_to_compare:\n",
        "    if word in polarites_verifiees:\n",
        "        print(f\"Le mot '{word}' a une polarité de {polarites_verifiees[word]}\")\n",
        "    else:\n",
        "        print(f\"Le mot '{word}' n'a pas été trouvé dans le fichier.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMCY8KFUVuHR",
        "outputId": "b21d6d3a-3aab-4bb1-8ab7-94c8e66c5545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le mot 'christian' a une polarité de -0.007\n",
            "Le mot 'jesus' a une polarité de -0.0\n",
            "Le mot 'church' a une polarité de 0.005\n",
            "Le mot 'muslim' a une polarité de -0.007\n",
            "Le mot 'jew' a une polarité de -0.0\n",
            "Le mot 'temple' a une polarité de 0.01\n",
            "Le mot 'faith' a une polarité de 0.14\n",
            "Le mot 'ritual' a une polarité de -0.018\n",
            "Le mot 'jesus' a une polarité de -0.0\n",
            "Le mot 'moses' a une polarité de 0.0\n",
            "Le mot 'mary' a une polarité de 0.0\n",
            "Le mot 'peace' a une polarité de 0.045\n",
            "Le mot 'forgiveness' a une polarité de 0.0\n",
            "Le mot 'grace' a une polarité de 0.0\n",
            "Le mot 'soul' a une polarité de -0.0\n"
          ]
        }
      ],
      "source": [
        "religious_words = [\n",
        "    \"christian\", \"jesus\", \"church\", \"muslim\",\n",
        "    \"jew\",\"temple\", \"faith\", \"ritual\",\n",
        "    \"jesus\",\"moses\", \"mary\", \"peace\", \"forgiveness\", \"grace\", \"soul\"\n",
        "]\n",
        "\n",
        "polarites_verifiees = {}\n",
        "\n",
        "with open(\"/content/result_out_clean_text.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        word, polarity, occurrences = line.split()\n",
        "        word = word.lower()  \n",
        "\n",
        "        # Si le mot est dans la liste des mots religieux, ajouter sa polarité\n",
        "        if word in religious_words:\n",
        "            polarites_verifiees[word] = float(polarity)\n",
        "\n",
        "for word in religious_words:\n",
        "    if word in polarites_verifiees:\n",
        "        print(f\"Le mot '{word}' a une polarité de {polarites_verifiees[word]}\")\n",
        "    else:\n",
        "        print(f\"Le mot '{word}' n'a pas été trouvé dans le fichier.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amoqVGCIW_J7",
        "outputId": "ff08a830-7a1e-4165-c353-2eecbef35c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le mot 'depression' a une polarité de -0.007\n",
            "Le mot 'stress' a une polarité de -0.01\n",
            "Le mot 'addiction' a une polarité de -0.013\n",
            "Le mot 'loneliness' a une polarité de -0.027\n",
            "Le mot 'guilt' a une polarité de 0.003\n",
            "Le mot 'anger' a une polarité de 0.0\n",
            "Le mot 'fear' a une polarité de 0.001\n",
            "Le mot 'frustration' a une polarité de 0.0\n",
            "Le mot 'treatment' a une polarité de -0.003\n",
            "Le mot 'suicide' a une polarité de -0.0\n"
          ]
        }
      ],
      "source": [
        "mental_health_words = [\n",
        "\"depression\",'stress',\"addiction\",\"loneliness\",\"guilt\",\"anger\",\"fear\",\"frustration\",\"treatment\",\"suicide\"\n",
        "]\n",
        "\n",
        "\n",
        "polarites_verifiees = {}\n",
        "\n",
        "with open(\"/content/result_out_clean_text.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        word, polarity, occurrences = line.split()\n",
        "        word = word.lower()  \n",
        "        # Si le mot est dans la liste des mots associés au racisme, ajouter sa polarité\n",
        "        if word in mental_health_words:\n",
        "            polarites_verifiees[word] = float(polarity)\n",
        "\n",
        "for word in mental_health_words:\n",
        "    if word in polarites_verifiees:\n",
        "        print(f\"Le mot '{word}' a une polarité de {polarites_verifiees[word]}\")\n",
        "    else:\n",
        "        print(f\"Le mot '{word}' n'a pas été trouvé dans le fichier.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
